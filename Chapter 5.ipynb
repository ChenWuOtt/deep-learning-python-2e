{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source -  https://www.kaggle.com/janiobachmann/bank-marketing-dataset. \n",
    "* The original dataset was sourced from UCI Machine Learning Repository and was contributed by [Moro et al., 2014]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-1  - Import required libraries\n",
    "#Import required libraries\n",
    "import torch.nn as nn\n",
    "import torch as tch\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import  precision_score, recall_score,roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-2  - Load data into memory\n",
    "#Load data into memory using pandas as a DataFrame\n",
    "df = pd.read_csv(\"Data/bank.csv\")\n",
    "print(\"DF Shape:\",df.shape)\n",
    "df.head() #print first 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-3  - Distribution of target values\n",
    "print(\"Distribution of Target Values in Dataset -\")\n",
    "df.deposit.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-4  - Distribution of na (null) values in dataset\n",
    "#Check if we have 'na' values within the dataset\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-5  - Distribution of distinct datatypes\n",
    "#Check the distinct datatypes within the dataset\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Listing 5-6  - Extract categorical columns from dataset\n",
    "#Extract categorical columns from dataset\n",
    "categorical_columns = df.select_dtypes(include=\"object\").columns #So, returns the names of all columns in the DataFrame df that contain text or categorical data.\n",
    "print(\"Categorical columns:\",list(categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each categorical column if values in (Yes/No) convert into a 1/0 Flag\n",
    "for col in categorical_columns:\n",
    "    if df[col].nunique() == 2:\n",
    "        df[col] = np.where(df[col]==\"yes\",1,0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-7  - Onehot encoding for remaining non-binary categorical variables\n",
    "#For the remaining cateogrical variables; \n",
    "#create one-hot encoded version of the dataset\n",
    "new_df = pd.get_dummies(df)\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define target and predictors for the model\n",
    "target = \"deposit\"\n",
    "predictors = list(set(new_df.columns) - set([target]))\n",
    "print(predictors)\n",
    "print(\"new_df.shape:\",new_df.shape)\n",
    "new_df[predictors].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-8  - Prepare data for training and validation\n",
    "\n",
    "#Convert all datatypes within pandas dataframe to Float32 \n",
    "#(Compatibility with PyTorch tensors)\n",
    "new_df = new_df.astype(np.float32)\n",
    "\n",
    "#Split dataset into Train/Test [70:30]\n",
    "X_train,X_temp, y_train,y_temp = train_test_split(new_df[predictors],new_df[target], test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the remaining data into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#Convert Pandas dataframe, first to numpy and then to Torch Tensors\n",
    "X_train = tch.from_numpy(X_train.values)\n",
    "X_test  = tch.from_numpy(X_test.values)\n",
    "X_val  = tch.from_numpy(X_val.values)\n",
    "y_train = tch.from_numpy(y_train.values).reshape(-1,1)\n",
    "y_test  = tch.from_numpy(y_test.values).reshape(-1,1)\n",
    "y_val  = tch.from_numpy(y_val.values).reshape(-1,1)\n",
    "\n",
    "#Print the dataset size to verify\n",
    "print(\"X_train.shape:\",X_train.shape)\n",
    "print(\"X_test.shape:\",X_test.shape)\n",
    "print(\"X_val.shape:\",X_val.shape)\n",
    "print(\"y_train.shape:\",y_train.shape)\n",
    "print(\"y_test.shape:\",y_test.shape)\n",
    "print(\"y_val.shape:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-9  - Define function to train model\n",
    "\n",
    "#Define function to train the network\n",
    "\n",
    "# 1. Function Definition: This line defines the function train_network with several parameters\n",
    "# def train_network(model,optimizer,loss_function,num_epochs,batch_size,X_train,Y_train,lambda_L1=0.0):\n",
    "def train_network(model, optimizer, loss_function, num_epochs, batch_size, X_train, Y_train, lambda_L1=0.0, X_val=None, Y_val=None):\n",
    "    \"\"\"\n",
    "    model: The neural network model to be trained.\n",
    "    optimizer: The optimizer used for updating the model’s weights.\n",
    "    loss_function: The function used to calculate the loss.\n",
    "    num_epochs: The number of epochs (iterations over the entire dataset) to train the model.\n",
    "    batch_size: The size of each training batch.\n",
    "    X_train, Y_train: The training data and corresponding labels.\n",
    "    lambda_L1: The L1 regularization parameter (default is 0.0).\n",
    "    X_val, Y_val: Optional validation data and labels.\n",
    "    \n",
    "    Return: \n",
    "    loss_across_epochs: List of training losses across epochs.\n",
    "    \"\"\"\n",
    "    # 2. Imports the tqdm library, which is used to display a progress bar during training.\n",
    "    # from tqdm import tqdm  # Import tqdm for progress bar\n",
    "    \n",
    "    # 3. Initialize List: Initializes an empty list to store the loss values for each epoch.   \n",
    "    loss_across_epochs = []\n",
    "    \n",
    "    # 4. Epoch Loop: Starts a loop that iterates over the number of epochs. Initializes train_loss to 0.0 for each epoch.\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss= 0.0\n",
    "        # 5. Set Model to Training Mode: Sets the model to training mode, which is necessary for certain layers like dropout\n",
    "        #Explicitly start model training\n",
    "        model.train()\n",
    "        # 6. Batch Loop with Progress Bar: Starts a loop that iterates over the training data in batches. The tqdm function displays a progress bar.\n",
    "        # for i in range(0,X_train.shape[0],batch_size):\n",
    "        for i in tqdm(range(0, X_train.shape[0], batch_size), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            \n",
    "            # 7. Extract Batch: Extracts a batch of input data and corresponding labels from the training set.\n",
    "            #Extract train batch from X and Y\n",
    "            input_data = X_train[i:min(X_train.shape[0],i+batch_size)]\n",
    "            labels = Y_train[i:min(X_train.shape[0],i+batch_size)]\n",
    "            # 8. Zero Gradients: Resets the gradients of the model parameters to zero before backpropagation.\n",
    "            #set the gradients to zero before starting to do backpropagation \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 9. Forward Pass: Passes the input data through the model to get the output.\n",
    "            output_data  = model(input_data)\n",
    "\n",
    "            # 10. Calculate Loss: Computes the loss between the model’s output and the true labels. Initializes L1_loss to 0.\n",
    "            loss = loss_function(output_data, labels)\n",
    "            L1_loss = 0\n",
    "            \n",
    "            # 11. Compute L1 Penalty: Iterates over the model parameters and computes the L1 penalty by summing the absolute values of the parameters.\n",
    "            for p in model.parameters():\n",
    "                L1_loss = L1_loss + p.abs().sum()  #gets abs() first and then does sum()           \n",
    "\n",
    "            # 12. Add L1 Penalty to Loss: Adds the L1 penalty to the original loss.\n",
    "            loss = loss + lambda_L1 * L1_loss\n",
    "\n",
    "            # 13. Backpropagation: Performs backpropagation to compute the gradients of the loss with respect to the model parameters.\n",
    "            loss.backward()\n",
    "\n",
    "            # 14. Update Weights: Updates the model parameters using the optimizer.\n",
    "            optimizer.step()\n",
    "\n",
    "            # 15. Accumulate Loss: Accumulates the loss for the current batch, scaled by the batch size.\n",
    "            train_loss += loss.item() * input_data.size(0)\n",
    "            \n",
    "        # 16. Store Epoch Loss: Appends the average loss for the epoch to the loss_across_epochs list.\n",
    "        loss_across_epochs.append(train_loss/X_train.size(0))\n",
    "        \n",
    "        # 17. Print Loss: Prints the loss every 500 epochs.\n",
    "        if epoch%500 == 0:\n",
    "            print(\"Epoch: {} - Loss:{:.4f}\".format(epoch,train_loss/X_train.size(0) ))   \n",
    "            \n",
    "        # 18. Validation Step: If validation data is provided, sets the model to evaluation mode, computes the validation loss, and prints it\n",
    "        if X_val is not None and Y_val is not None:\n",
    "            model.eval()\n",
    "            with tch.no_grad():\n",
    "                val_output = model(X_val)\n",
    "                val_loss = loss_function(val_output, Y_val)\n",
    "                print(f\"Validation Loss: {val_loss.item():.4f}\")    \n",
    "                \n",
    "    # 19. Return Losses: Returns the list of training losses across epochs.   \n",
    "    return(loss_across_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-10  - Define function to evaluate model\n",
    "\n",
    "#Define function for evaluating NN\n",
    "def evaluate_model(model,X_test,y_test,X_train,y_train,loss_list):\n",
    "    # 1. This sets the model to evaluation mode, which disables certain layers like dropout and batch normalization that are only used during training.\n",
    "    model.eval() #Explicitly set to evaluate mode\n",
    "\n",
    "    # 2. Predict on Train and Validation Datasets\n",
    "    # y_test_prob and y_train_prob are the predicted probabilities for the test and training datasets.\n",
    "    # y_test_pred and y_train_pred convert these probabilities into binary predictions (0 or 1) based on a threshold of 0.5.\n",
    "    y_test_prob = model(X_test)\n",
    "    y_test_pred = np.where(y_test_prob>0.5,1,0)\n",
    "    y_train_prob = model(X_train)\n",
    "    y_train_pred = np.where(y_train_prob>0.5,1,0)\n",
    "\n",
    "    # 3. Compute Training and Validation Metrics\n",
    "    # This block prints the performance metrics for both training and validation datasets, including accuracy, precision, recall, and ROC AUC scores.\n",
    "    print(\"\\n Model Performance -\")\n",
    "    print(\"Training Accuracy--\",-round(accuracy_score(y_train,y_train_pred),3))\n",
    "    print(\"Training Precision-\",-round(precision_score(y_train,y_train_pred),3))\n",
    "    print(\"Training Recall----\",-round(recall_score(y_train,y_train_pred),3))\n",
    "    print(\"Training ROCAUC----\", round(roc_auc_score(y_train,y_train_prob.detach().numpy()),3))\n",
    "\n",
    "    print(\"Validation Accuracy--\",-round(accuracy_score(y_test,y_test_pred),3))\n",
    "    print(\"Validation Precision-\",-round(precision_score(y_test,y_test_pred),3))\n",
    "    print(\"Validation Recall----\",-round(recall_score(y_test,y_test_pred),3))\n",
    "    print(\"Validation ROCAUC----\", round(roc_auc_score(y_test,y_test_prob.detach().numpy()),3))    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # 4. Plot the Loss curve and ROC Curve\n",
    "    # This creates a figure with two subplots.\n",
    "    #    The first subplot shows the loss curve across epoch\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_list)\n",
    "    plt.title('Loss across epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    \n",
    "    #   The second subplot shows the ROC curves for both validation and training datasets.\n",
    "    plt.subplot(1, 2, 2)\n",
    "    #Validation\n",
    "    # roc_curve computes the false positive rate (FPR) and true positive rate (TPR) for different threshold values\n",
    "    fpr_v, tpr_v, _ = roc_curve(y_test, y_test_prob.detach().numpy())\n",
    "    # auc calculates the area under the ROC curve (AUC).\n",
    "    roc_auc_v = auc(fpr_v, tpr_v)\n",
    "    \n",
    "    #Training\n",
    "    # roc_curve computes the false positive rate (FPR) and true positive rate (TPR) for different threshold values\n",
    "    fpr_t, tpr_t, _ = roc_curve(y_train, y_train_prob.detach().numpy())\n",
    "    # auc calculates the area under the ROC curve (AUC).\n",
    "    roc_auc_t = auc(fpr_t, tpr_t)    \n",
    "    plt.title('Receiver Operating Characteristic:Validation')\n",
    "    plt.plot(fpr_v, tpr_v, 'b', label = 'Validation AUC = %0.2f' % roc_auc_v)\n",
    "    plt.plot(fpr_t, tpr_t, 'r', label = 'Training AUC = %0.2f' % roc_auc_t)    \n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-11  - Define Neural Network\n",
    "\n",
    "#Define Neural Network\n",
    "# 1. class NeuralNetwork(nn.Module): This line defines a new class NeuralNetwork that inherits from nn.Module, which is a base class for all neural network modules in PyTorch.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # 2. This is the constructor method that initializes the neural network.\n",
    "    def __init__(self):\n",
    "        # 3.  Calls the constructor of the parent class nn.Module.\n",
    "        super().__init__()\n",
    "        # 4. Sets the random seed for reproducibility. This ensures that the results are the same every time the code is run.\n",
    "        tch.manual_seed(2020)\n",
    "        # 5. Defines the first fully connected layer with 48 input features and 96 output features.\n",
    "        self.fc1 = nn.Linear(48, 96) \n",
    "        # 6. Defines the second fully connected layer with 96 input features and 192 output features.\n",
    "        self.fc2 = nn.Linear(96, 192)\n",
    "        # 7.  Defines the third fully connected layer with 192 input features and 384 output features.\n",
    "        self.fc3 = nn.Linear(192, 384)\n",
    "        # 8. Defines the output layer with 384 input features and 1 output feature.\n",
    "        self.out = nn.Linear(384, 1) \n",
    "        # 9. Defines the ReLU activation function.       \n",
    "        self.relu = nn.ReLU()     \n",
    "        # 10 Defines the Sigmoid activation function for the final output.   \n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "    # 11. def forward(self, x): Defines the forward pass of the neural network. This method takes an input tensor x and returns the output y.    \n",
    "    def forward(self, x):\n",
    "        op = self.fc1(x)   # Passes the input x through the first fully connected layer.\n",
    "        op = self.relu(op) # Applies the ReLU activation function to the output of the first layer.       \n",
    "        op = self.fc2(op)  # Passes the result through the second fully connected layer.\n",
    "        op = self.relu(op) # Applies the ReLU activation function to the output of the second layer.\n",
    "        op = self.fc3(op)  # Passes the result through the third fully connected layer.\n",
    "        op = self.relu(op) # Applies the ReLU activation function to the output of the third layer.\n",
    "        op = self.out(op)  # Passes the result through the output layer.\n",
    "        y = self.final(op) # Applies the Sigmoid activation function to the output.\n",
    "        return y\n",
    "    \n",
    "#Define training variables\n",
    "num_epochs = 256\n",
    "batch_size= 64\n",
    "loss_function = nn.BCELoss()  #Binary Crosss Entropy Loss\n",
    "\n",
    "#Hyperparameters\n",
    "weight_decay=0.0 #set to 0; no L2 Regularizer; passed into the Optimizer\n",
    "lambda_L1=0.0    #Set to 0; no L1 reg; manually added in loss (train_network)\n",
    "\n",
    "#Create a model instance\n",
    "model = NeuralNetwork()\n",
    "\n",
    "#Define optimizer\n",
    "adam_optimizer = tch.optim.Adam(model.parameters(), lr= 0.001,weight_decay=weight_decay)\n",
    "\n",
    "#Train model\n",
    "adam_loss = train_network(model,adam_optimizer,loss_function,num_epochs,batch_size,X_train,y_train,lambda_L1,X_val,y_val)\n",
    "\n",
    "#Evaluate model\n",
    "evaluate_model(model,X_test,y_test,X_train,y_train,adam_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-12  - L1 Regularization\n",
    "#L1 Regularization    \n",
    "num_epochs = 256\n",
    "batch_size= 64\n",
    "\n",
    "weight_decay=0.0   #Set to 0; no L2 reg\n",
    "lambda_L1 = 0.0001 #Enables L1 Regularization\n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_function = nn.BCELoss()  #Binary Crosss Entropy Loss\n",
    "\n",
    "adam_optimizer = tch.optim.Adam(model.parameters(),lr= 0.001 ,weight_decay=weight_decay)\n",
    "\n",
    "#Define hyperparameter for L1 Regularization\n",
    "#Train network\n",
    "adam_loss = train_network(model,adam_optimizer,loss_function ,num_epochs,batch_size,X_train,y_train,lambda_L1=lambda_L1)\n",
    "\n",
    "#Evaluate model\n",
    "evaluate_model(model,X_test,y_test,X_train,y_train,adam_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-13  - L2 Regularization\n",
    "\n",
    "#L2 Regularization    \n",
    "num_epochs = 256\n",
    "batch_size= 64\n",
    "weight_decay =0.001 # Enables L2 Regularization\t\n",
    "lambda_L1 = 0.00    # Set to 0; no L1 reg\n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_function = nn.BCELoss()  #Binary Crosss Entropy Loss\n",
    "\n",
    "adam_optimizer = tch.optim.Adam(model.parameters(),lr= 0.001,weight_decay=weight_decay)\n",
    "\n",
    "# The Adam optimizer in PyTorch has several key parameters that you can adjust to fine-tune the training of your neural network. Here are the main parameters:\n",
    "# params: The parameters to optimize, typically passed as model.parameters().\n",
    "# lr (learning rate): The step size for updating the weights. Default is 0.001.\n",
    "# betas: A tuple (beta1, beta2) that represents the coefficients used for computing running averages of gradient and its square. Default is (0.9, 0.999).\n",
    "# eps (epsilon): A small constant for numerical stability. Default is 1e-8.\n",
    "# weight_decay: A value for L2 regularization (weight decay). Default is 0.\n",
    "# amsgrad: A boolean indicating whether to use the AMSGrad variant of this algorithm. Default is False.\n",
    "# \n",
    "#Train Network\n",
    "adam_loss = train_network(model,adam_optimizer,loss_function,num_epochs,batch_size,X_train,y_train,lambda_L1=lambda_L1)\n",
    "\n",
    "#Evaluate model\n",
    "evaluate_model(model,X_test,y_test,X_train,y_train,adam_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-14  - Dropout Regularization\n",
    "\n",
    "#Define Network with Dropout Layers\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # Adding droput layers within Neural Network to reduce overfitting\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        tch.manual_seed(2020)\n",
    "        self.fc1 = nn.Linear(48, 96)\n",
    "        self.fc2 = nn.Linear(96, 192)\n",
    "        self.fc3 = nn.Linear(192, 384)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(384, 1)\n",
    "        self.final = nn.Sigmoid()\n",
    "        self.drop = nn.Dropout(p=0.1)  # # Dropout with a probability of 0.1\n",
    "\n",
    "        # Yes, you can place dropout layers at various points in your forward pass. However, it’s typically used after fully connected (dense) layers or convolutional layers to prevent overfitting by randomly setting a fraction of the input units to zero during training.\n",
    "        # Here are a few guidelines to consider:\n",
    "            # After Activation Functions: Dropout is often applied after activation functions like ReLU. This helps in regularizing the activations.\n",
    "            # Between Layers: You can place dropout layers between fully connected layers to ensure that the network does not rely too heavily on any particular neurons.\n",
    "            # Not Before Output Layer: It’s generally not recommended to apply dropout before the output layer, especially if you’re using a softmax or sigmoid activation for classification tasks, as it can lead to unstable outputs.      \n",
    "\n",
    "    def forward(self, x):\n",
    "        op = self.drop(x)  # Dropout for input layer\n",
    "        op = self.fc1(op)\n",
    "        op = self.relu(op)        \n",
    "        op = self.drop(op) # Dropout for hidden layer 1\n",
    "        op = self.fc2(op)\n",
    "        op = self.relu(op)\n",
    "        op = self.drop(op) # Dropout for hidden layer 2\n",
    "        op = self.fc3(op)\n",
    "        op = self.relu(op)      \n",
    "        op = self.drop(op) # Dropout for hidden layer 3       \n",
    "        op = self.out(op)\n",
    "        y = self.final(op)\n",
    "        return y\n",
    "    \n",
    "num_epochs = 256\n",
    "batch_size= 64\n",
    "\n",
    "weight_decay=0.0 #Set to 0; no L2 reg\n",
    "lambda_L1 = 0.0  #Set to 0; no L1 reg\n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_function = nn.BCELoss()  #Binary Crosss Entropy Loss\n",
    "\n",
    "adam_optimizer = tch.optim.Adam(model.parameters(),lr= 0.001,weight_decay=weight_decay)\n",
    "#Train model\n",
    "adam_loss = train_network(model,adam_optimizer,loss_function,num_epochs,batch_size,X_train,y_train,lambda_L1= lambda_L1)\n",
    "\n",
    "#Evaluate model\n",
    "evaluate_model(model,X_test,y_test,X_train,y_train,adam_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 5-15  - L1, L2 + Dropout Regularization\n",
    "\n",
    "\n",
    "#Create a network with Dropout layer\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        tch.manual_seed(2020)\n",
    "        self.fc1 = nn.Linear(48, 96)\n",
    "        self.fc2 = nn.Linear(96, 192)\n",
    "        self.fc3 = nn.Linear(192, 384)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(384, 1)\n",
    "        self.final = nn.Sigmoid()\n",
    "        self.drop = nn.Dropout(0.1)  #Dropout Layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        op = self.drop(x)  #Dropout for input layer\n",
    "        op = self.fc1(op)\n",
    "        op = self.relu(op)        \n",
    "        op = self.drop(op) #Dropout for hidden layer 1\n",
    "        op = self.fc2(op)\n",
    "        op = self.relu(op)\n",
    "        op = self.drop(op) #Dropout for hidden layer 2\n",
    "        op = self.fc3(op)\n",
    "        op = self.relu(op)      \n",
    "        op = self.drop(op) #Dropout for hidden layer 3       \n",
    "        op = self.out(op)\n",
    "        y = self.final(op)\n",
    "        return y\n",
    "    \n",
    "num_epochs = 256\n",
    "batch_size= 64\n",
    "\n",
    "lambda_L1    = 0.0001  #Enabled L1 \n",
    "weight_decay =0.001    #Enabled L2\n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "adam_optimizer = tch.optim.Adam(model.parameters(),lr= 0.001 ,weight_decay=weight_decay)\n",
    "\n",
    "adam_loss = train_network(model,adam_optimizer,loss_function ,num_epochs,batch_size,X_train,y_train,lambda_L1=lambda_L1)\n",
    "\n",
    "evaluate_model(model,X_test,y_test,X_train,y_train,adam_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
